{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7acc8ca",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial\n",
    "\n",
    "**William Yue**\n",
    "\n",
    "In this Jupyter notebook, my goal is to gain familiarity with PyTorch by following the [online tutorials](https://pytorch.org/tutorials/). Hopefully I will know how it works at the end.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to Pytorch](#Introduction-to-PyTorch)<br />\n",
    "    1.1 [Tensors](#Tensors)<br />\n",
    "    1.2 [Datasets & DataLoaders](#Datasets-and-DataLoaders)<br />\n",
    "    1.3 [Summary of Section 1](#Summary-of-Section-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b56f409",
   "metadata": {},
   "source": [
    "## ยง1 Introduction to PyTorch\n",
    "\n",
    "### 1.1 Tensors\n",
    "\n",
    "Let's start by getting `torch` and `numpy` in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "822745ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be15c71",
   "metadata": {},
   "source": [
    "Tensors appear to just be `torch`'s version of a matrix or multi-dimensional array, similar to `numpy`'s ndarrays. The difference is that tensors can run on GPUs or other fast hardware. They are also optimized for automatic differentiation.\n",
    "\n",
    "#### Initializing a Tensor\n",
    "\n",
    "There are several ways to make a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "033f797a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "data = [[1,2,3],[4,5,6]]\n",
    "x_data = torch.tensor(data)\n",
    "print(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba761585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "np_data = np.arange(6).reshape(2,3)\n",
    "x_np = torch.tensor(np_data)\n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7455102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]])\n",
      "tensor([[0.7008, 0.4108, 0.3072],\n",
      "        [0.0762, 0.3754, 0.9614]])\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data)\n",
    "print(x_ones)\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float)\n",
    "print(x_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61c4501",
   "metadata": {},
   "source": [
    "Note that we have a problem if we don't convert the `dtype` in the `torch.rand_like` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "573fd5cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\"check_uniform_bounds\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21824/2854645661.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_rand_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: \"check_uniform_bounds\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "x_rand_test = torch.rand_like(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34295942",
   "metadata": {},
   "source": [
    "This appears to be because the initial tensor `x_data` has the datatype `long` (64-bit integer, according to [documentation](https://pytorch.org/docs/stable/tensor_attributes.html)), and there is no way to sample a random number in the interval `[0,1)` for this datatype.\n",
    "\n",
    "We can also directly specify the shape for `torch.rand`, `torch.ones`, and `torch.zeros`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bbc4cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5223, 0.4205, 0.5046],\n",
      "        [0.7249, 0.7542, 0.8844]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "shape=(2,3)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(rand_tensor,ones_tensor,zeros_tensor,sep='\\n')\n",
    "\n",
    "another_ones_tensor = torch.ones(4,5)\n",
    "print(another_ones_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4edc424",
   "metadata": {},
   "source": [
    "There are several attributes of tensors that we can check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bb0475d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.float32\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(tensor.shape)\n",
    "print(tensor.dtype)\n",
    "print(tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3160191",
   "metadata": {},
   "source": [
    "#### Operations on Tensors\n",
    "\n",
    "If you're observant, you'll notice that the device above that the tensor is stored on is a CPU! It turns out that by default, all tensors are initialized with `cpu` as their device. I'm on a Makerspace computer that comes with a NVIDIA GPU that supports CUDA, so we'll want to convert the device to a GPU possible. We can first check if CUDA is available before switching the tensor to that using the `.to` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2bbb014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1523, 0.0669, 0.9925, 0.9683],\n",
      "        [0.2053, 0.0651, 0.3430, 0.8538],\n",
      "        [0.2522, 0.8512, 0.3672, 0.4854]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')\n",
    "\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a816e089",
   "metadata": {},
   "source": [
    "Looks better now!\n",
    "\n",
    "Tensors can be operated on similar to `numpy` arrays, with standard indexing and slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6da2dea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9981, 0.7904, 0.4151, 0.4267],\n",
      "        [0.6463, 0.6392, 0.4745, 0.2393],\n",
      "        [0.8886, 0.1257, 0.5814, 0.9456],\n",
      "        [0.7998, 0.0311, 0.5486, 0.9827]])\n",
      "tensor([0.9981, 0.7904, 0.4151, 0.4267])\n",
      "tensor([[0.9981, 0.7904, 0.4151, 0.4267]])\n",
      "tensor([0.9981, 0.6463, 0.8886, 0.7998])\n",
      "tensor([0.4267, 0.2393, 0.9456, 0.9827])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(4,4)\n",
    "print(tensor)\n",
    "\n",
    "print(tensor[0])\n",
    "print(tensor[0:1])\n",
    "print(tensor[:,0])\n",
    "print(tensor[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cc0e8a",
   "metadata": {},
   "source": [
    "Note that similar to `numpy` arrays, `tensor[0]` and `tensor[0:1]` have different dimensionalities. We can also do standard concatenation along a given `dim` (not `axis`) using `torch.cat`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b6e8d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9981, 0.7904, 0.4151, 0.4267],\n",
      "        [0.6463, 0.6392, 0.4745, 0.2393],\n",
      "        [0.8886, 0.1257, 0.5814, 0.9456],\n",
      "        [0.7998, 0.0311, 0.5486, 0.9827],\n",
      "        [0.9981, 0.7904, 0.4151, 0.4267],\n",
      "        [0.6463, 0.6392, 0.4745, 0.2393],\n",
      "        [0.8886, 0.1257, 0.5814, 0.9456],\n",
      "        [0.7998, 0.0311, 0.5486, 0.9827],\n",
      "        [0.9981, 0.7904, 0.4151, 0.4267],\n",
      "        [0.6463, 0.6392, 0.4745, 0.2393],\n",
      "        [0.8886, 0.1257, 0.5814, 0.9456],\n",
      "        [0.7998, 0.0311, 0.5486, 0.9827]])\n",
      " ----- \n",
      "tensor([[0.9981, 0.7904, 0.4151, 0.4267, 0.9981, 0.7904, 0.4151, 0.4267, 0.9981,\n",
      "         0.7904, 0.4151, 0.4267],\n",
      "        [0.6463, 0.6392, 0.4745, 0.2393, 0.6463, 0.6392, 0.4745, 0.2393, 0.6463,\n",
      "         0.6392, 0.4745, 0.2393],\n",
      "        [0.8886, 0.1257, 0.5814, 0.9456, 0.8886, 0.1257, 0.5814, 0.9456, 0.8886,\n",
      "         0.1257, 0.5814, 0.9456],\n",
      "        [0.7998, 0.0311, 0.5486, 0.9827, 0.7998, 0.0311, 0.5486, 0.9827, 0.7998,\n",
      "         0.0311, 0.5486, 0.9827]])\n"
     ]
    }
   ],
   "source": [
    "t0 = torch.cat([tensor]*3, dim=0)\n",
    "t1 = torch.cat([tensor]*3, dim=1)\n",
    "\n",
    "print(t0,t1,sep='\\n ----- \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625ddbb6",
   "metadata": {},
   "source": [
    "We also have standard arithmetic operations. Three ways to do matrix multiplication are shown below, using `@` and `matmul`. `y1`, `y2`, and `y3` should have the same value. Note that `.T` transposes the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25e30a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.9753, 1.4494, 1.6310, 1.4699],\n",
       "        [1.4494, 1.1087, 1.1568, 1.0323],\n",
       "        [1.6310, 1.1568, 2.0375, 1.9628],\n",
       "        [1.4699, 1.0323, 1.9628, 1.9074]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "y3 = torch.rand(tensor.shape)\n",
    "torch.matmul(tensor, tensor.T, out=y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41b6cf5",
   "metadata": {},
   "source": [
    "If you want to do element-wise multiplication instead, you can use `*` or `mul` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de77f76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9627e-01, 6.2471e-01, 1.7228e-01, 1.8208e-01],\n",
       "        [4.1770e-01, 4.0859e-01, 2.2516e-01, 5.7284e-02],\n",
       "        [7.8954e-01, 1.5800e-02, 3.3798e-01, 8.9415e-01],\n",
       "        [6.3963e-01, 9.6775e-04, 3.0098e-01, 9.6579e-01]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "z3 = torch.rand(tensor.shape) # note that using torch.rand_like also works\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c72474",
   "metadata": {},
   "source": [
    "If your tensor has one element (for example if you summed everything in the tensor), you can get that element out using `.item()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a1dbeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.5331)\n",
      "<class 'torch.Tensor'>\n",
      "9.533109664916992\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum()\n",
    "print(agg, type(agg), sep='\\n')\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b076abfe",
   "metadata": {},
   "source": [
    "### Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95735127",
   "metadata": {},
   "source": [
    "### Summary of Section 1\n",
    "\n",
    "**1.1 Tensors**\n",
    "* Tensors are PyTorch's version of matrices, similar to `numpy`'s ndarrays.\n",
    "* There are many ways to create them: `torch.tensor()`, `torch.from_numpy()`, `torch.rand_like()`, and `torch.ones()`.\n",
    "* We can check if CUDA is on our computer using `torch.cuda.is_available()`. Tensors are normally initialized with device `cpu`, so we need to convert them to CUDA using `.to('cuda')`.\n",
    "* Tensors can be sliced like `numpy` arrays using similar operations. There's normal slicing, `tensor.cat` for joining tensors, across dimension `dim`, and standard matrix multiplication or element-wise multiplication by `@` or `*`, respectively (`matmul` and `mul` also work, respectively).\n",
    "* Given a tensor of a single element, we can use `.item()` to extract it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
